{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Training Operation for Llama 3.1 Fine-tuning\n",
        "\n",
        "This notebook contains the fine-tuning logic for Llama 3.1 models that will be embedded into the KFP pipeline.\n",
        "\n",
        "## Workflow\n",
        "1. Edit and test your training code in this notebook\n",
        "2. Build the pipeline using the build script\n",
        "3. Submit the generated pipeline YAML to your pipeline orchestrator\n",
        "\n",
        "## Key Function\n",
        "The main `trainOp()` function is what gets embedded into the KFP component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trainop-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainOp(data_name: str = 'financial_sentiment_data.jsonl',\n",
        "            data_source: str = '@auto-populate-data-source',\n",
        "            data_bucket_prefix: str = '@auto-populate-object-prefix', \n",
        "            model_relative_path: str = 'model', \n",
        "            model_name: str = 'llama31-financial-sentiment',\n",
        "            model_id_or_path: str = 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "            model_cache_dir: str = '@auto-populate-modelcache-path',\n",
        "            num_train_epochs: float = 1,\n",
        "            per_device_train_batch_size: int = 4, \n",
        "            gradient_accumulation_steps: int = 4, \n",
        "            learning_rate: float = 2e-4,\n",
        "            max_length: int = 1024,\n",
        "            lora_rank: int = 16,\n",
        "            lora_alpha: int = 32,\n",
        "            precision: str = 'bf16',\n",
        "            use_quantization: bool = True,\n",
        "            full_finetune: bool = False,\n",
        "            quantization_type: str = 'nf4',\n",
        "            model_version: str = '@auto-timestamp',\n",
        "            kfp_output_path: str = None):\n",
        "    \"\"\"\n",
        "    Main entry point for fine-tuning and saving a LLM model.\n",
        "    \n",
        "    Args:\n",
        "        data_name (str): Relative name for input data, default 'financial_sentiment_data.jsonl'\n",
        "        data_source (str): Data source ('minio' or 'gcs'), default '@auto-populate-data-source' (auto-populated by caller based on environment)\n",
        "        data_bucket_prefix (str): Object storage path prefix (default: '@auto-populate-object-prefix' - will be auto-populated by caller)\n",
        "                                 Format: gs://{bucket-name}/{project-id}/ or s3://{bucket-name}/{project-id}/\n",
        "                                 Usage: data_bucket_prefix + data_name (e.g., 'gs://my-bucket/my-project/financial_sentiment_data.jsonl')\n",
        "        model_relative_path (str): Relative path for model storage, default 'model'\n",
        "        model_name (str): Name of the model, default 'llama31-financial-sentiment'\n",
        "        model_id_or_path (str): Relative path for model path, default '/mnt/pretrained-models/'\n",
        "        num_train_epochs (float): Number of training epochs, default 1\n",
        "        per_device_train_batch_size (int): Number of train batch size, default 4\n",
        "        gradient_accumulation_steps (int): Number of gradient accumulation steps, default 4\n",
        "        learning_rate (float): Number of lr, default 2e-4\n",
        "        max_length (int): The maximum length the generated tokens can have, default 1024\n",
        "        lora_rank (int): Lora attention dimension (the \"rank\"), default 16\n",
        "        lora_alpha (int): The alpha parameter for Lora scaling, default 32\n",
        "        precision (str): Training precision (fp32, bf16, or fp16), default 'bf16'\n",
        "        use_quantization (bool): Use 4-bit quantization (useful for smaller GPUs), default True\n",
        "        full_finetune (bool): Do full fine-tuning instead of LoRA (requires more VRAM), default False\n",
        "        quantization_type (str): Quantization type, default 'nf4'\n",
        "        model_version (str): Model version number (default: '@auto-timestamp' - will be auto-generated by caller)\n",
        "        kfp_output_path (str): KFP v2 Output[Model] artifact path (auto-uploaded to GCS on Vertex AI)\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import logging\n",
        "    import subprocess\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import zipfile\n",
        "    from typing import Dict, List, Any\n",
        "    \n",
        "    home = '/home/jovyan'\n",
        "    input_dir = os.path.join(home, 'data')\n",
        "    \n",
        "    # Create input directory\n",
        "    os.makedirs(input_dir, exist_ok=True)\n",
        "    \n",
        "    # Set up logging\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    # Use KFP output path if provided (for Vertex AI auto-upload to GCS)\n",
        "    # Otherwise use local path (for local testing/runs)\n",
        "    if kfp_output_path:\n",
        "        output_dir = kfp_output_path\n",
        "        logger.info(f\"Using KFP Output artifact path: {output_dir}\")\n",
        "        logger.info(\"Model will be automatically uploaded to GCS by Vertex AI\")\n",
        "    else:\n",
        "        output_dir = os.path.join(home, model_relative_path, model_name)\n",
        "        logger.info(f\"Using local path: {output_dir}\")\n",
        "\n",
        "    # ========== GPU DEVICE AVAILABILITY CHECK ==========\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CHECKING GPU DEVICE AVAILABILITY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Check /dev for GPU devices\n",
        "    try:\n",
        "        result = subprocess.run(['ls', '-la', '/dev'], capture_output=True, text=True)\n",
        "        gpu_devices = [line for line in result.stdout.split('\\n') if 'nvidia' in line.lower()]\n",
        "        if gpu_devices:\n",
        "            print(f\"\u2713 Found {len(gpu_devices)} NVIDIA device(s):\")\n",
        "            for dev in gpu_devices:\n",
        "                print(f\"  {dev}\")\n",
        "        else:\n",
        "            print(\"\u2717 NO NVIDIA devices found in /dev!\")\n",
        "            print(\"This means GPU is NOT mounted by Vertex AI\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking devices: {e}\")\n",
        "\n",
        "    # Check environment\n",
        "    print(\"\\nNVIDIA environment variables:\")\n",
        "    for key in ['NVIDIA_VISIBLE_DEVICES', 'NVIDIA_DRIVER_CAPABILITIES', 'NVIDIA_REQUIRE_CUDA']:\n",
        "        print(f\"  {key}={os.environ.get(key, 'NOT SET')}\")\n",
        "\n",
        "    # ========== GPU DEBUGGING INFORMATION ==========\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(\"GPU INFORMATION\")\n",
        "    logger.info(\"=\" * 80)\n",
        "\n",
        "    # Basic CUDA availability\n",
        "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Number of GPUs\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        logger.info(f\"Number of GPUs detected: {gpu_count}\")\n",
        "\n",
        "        # Current GPU\n",
        "        current_device = torch.cuda.current_device()\n",
        "        logger.info(f\"Current CUDA device: {current_device}\")\n",
        "\n",
        "        # Detailed info for each GPU\n",
        "        for i in range(gpu_count):\n",
        "            logger.info(f\"\\n--- GPU {i} Details ---\")\n",
        "            logger.info(f\"  Name: {torch.cuda.get_device_name(i)}\")\n",
        "\n",
        "            # Get device properties\n",
        "            props = torch.cuda.get_device_properties(i)\n",
        "            logger.info(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
        "            logger.info(f\"  Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
        "            logger.info(f\"  Multi Processor Count: {props.multi_processor_count}\")\n",
        "\n",
        "            # Memory info\n",
        "            logger.info(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
        "            logger.info(f\"  Memory Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
        "            logger.info(f\"  Max Memory Allocated: {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")\n",
        "            logger.info(f\"  Max Memory Reserved: {torch.cuda.max_memory_reserved(i) / 1024**3:.2f} GB\")\n",
        "\n",
        "        # CUDA version\n",
        "        logger.info(f\"\\nCUDA Version: {torch.version.cuda}\")\n",
        "        logger.info(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
        "        logger.info(f\"cuDNN Enabled: {torch.backends.cudnn.enabled}\")\n",
        "\n",
        "        # Try running nvidia-smi\n",
        "        logger.info(\"\\n--- NVIDIA-SMI Output ---\")\n",
        "        try:\n",
        "            result = subprocess.run(['nvidia-smi'],\n",
        "                                  capture_output=True,\n",
        "                                  text=True,\n",
        "                                  timeout=10)\n",
        "            if result.returncode == 0:\n",
        "                logger.info(f\"\\n{result.stdout}\")\n",
        "            else:\n",
        "                logger.warning(f\"nvidia-smi failed with code {result.returncode}: {result.stderr}\")\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"nvidia-smi command not found\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            logger.warning(\"nvidia-smi command timed out\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to run nvidia-smi: {e}\")\n",
        "\n",
        "        # Check GPU visibility from environment variables\n",
        "        logger.info(\"\\n--- GPU Environment Variables ---\")\n",
        "        gpu_env_vars = ['CUDA_VISIBLE_DEVICES', 'NVIDIA_VISIBLE_DEVICES',\n",
        "                       'CUDA_DEVICE_ORDER', 'CUDA_LAUNCH_BLOCKING']\n",
        "        for var in gpu_env_vars:\n",
        "            value = os.environ.get(var, 'Not set')\n",
        "            logger.info(f\"  {var}: {value}\")\n",
        "    else:\n",
        "        logger.warning(\"CUDA is not available! Training will run on CPU (very slow).\")\n",
        "        logger.info(\"\\nPossible issues:\")\n",
        "        logger.info(\"  1. No GPU present in the container\")\n",
        "        logger.info(\"  2. NVIDIA drivers not installed on host\")\n",
        "        logger.info(\"  3. NVIDIA Container Toolkit not configured\")\n",
        "        logger.info(\"  4. GPU not mounted/passed to container\")\n",
        "\n",
        "        # Check for nvidia-smi anyway to see if drivers are present\n",
        "        try:\n",
        "            result = subprocess.run(['nvidia-smi'],\n",
        "                                  capture_output=True,\n",
        "                                  text=True,\n",
        "                                  timeout=10)\n",
        "            if result.returncode == 0:\n",
        "                logger.info(\"\\nnvidia-smi is available but PyTorch can't see CUDA!\")\n",
        "                logger.info(\"This usually means PyTorch was not built with CUDA support.\")\n",
        "                logger.info(f\"\\n{result.stdout}\")\n",
        "            else:\n",
        "                logger.info(\"\\nnvidia-smi not available or failed\")\n",
        "        except:\n",
        "            logger.info(\"\\nnvidia-smi command not found\")\n",
        "\n",
        "    logger.info(\"=\" * 80)\n",
        "    logger.info(f\"Model ID or path: {model_id_or_path}\")\n",
        "    logger.info(f\"Model cache directory: {model_cache_dir}\")\n",
        "    logger.info(f\"Model version: {model_version}\")\n",
        "    logger.info(f\"Data source: {data_source}\")\n",
        "    logger.info(f\"Data bucket prefix: {data_bucket_prefix}\")\n",
        "    logger.info(\"=\" * 80)\n",
        "    \n",
        "    def download_data():\n",
        "        \"\"\"Download fine-tuning data from MinIO or GCS.\"\"\"\n",
        "        if data_source == 'gcs':\n",
        "            # Download from GCS using google-cloud-storage Python library\n",
        "            # Construct full GCS path: data_bucket_prefix + data_name\n",
        "            gcs_full_path = f\"{data_bucket_prefix.rstrip('/')}/{data_name}\"\n",
        "            local_path = os.path.join(input_dir, data_name)\n",
        "\n",
        "            logger.info(f'Downloading data from GCS: {gcs_full_path}')\n",
        "            logger.info(f'Destination: {local_path}')\n",
        "\n",
        "            try:\n",
        "                from google.cloud import storage\n",
        "\n",
        "                # Parse gs://bucket-name/path/to/file\n",
        "                if not gcs_full_path.startswith('gs://'):\n",
        "                    raise ValueError(f\"Invalid GCS path: {gcs_full_path}\")\n",
        "\n",
        "                path_parts = gcs_full_path[5:].split('/', 1)  # Remove 'gs://' and split\n",
        "                bucket_name = path_parts[0]\n",
        "                blob_name = path_parts[1] if len(path_parts) > 1 else ''\n",
        "\n",
        "                logger.info(f\"Bucket: {bucket_name}, Blob: {blob_name}\")\n",
        "\n",
        "                # Initialize GCS client (uses Application Default Credentials)\n",
        "                client = storage.Client()\n",
        "                bucket = client.bucket(bucket_name)\n",
        "                blob = bucket.blob(blob_name)\n",
        "\n",
        "                # Download the file\n",
        "                blob.download_to_filename(local_path)\n",
        "                logger.info('Download from GCS completed successfully')\n",
        "\n",
        "            except ImportError:\n",
        "                logger.error('google-cloud-storage library not found. Install with: pip install google-cloud-storage')\n",
        "                raise\n",
        "            except Exception as e:\n",
        "                logger.error(f'Failed to download from GCS: {e}')\n",
        "                raise\n",
        "        else:\n",
        "            # Download from MinIO (default)\n",
        "            from tintin.file.minio import FileManager as FileManager\n",
        "            logger.info(f'Downloading data from MinIO: {data_name}')\n",
        "            logger.info(f'Destination: {input_dir}')\n",
        "            debug = False\n",
        "            recursive = False\n",
        "            mgr = FileManager('', debug)\n",
        "            mgr.download(input_dir, [data_name], recursive)\n",
        "            logger.info('Download from MinIO completed')\n",
        "\n",
        "    def load_jsonl_data(file_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load training examples from a jsonl file.\"\"\"\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line.strip()))\n",
        "        logger.info(f\"Loaded {len(data)} examples from {file_path}\")\n",
        "        return data\n",
        "    \n",
        "    def format_instruction(example: Dict[str, str]) -> str:\n",
        "        \"\"\"Format the instruction, input, and output for Llama-3.1 chat format.\"\"\"\n",
        "        system_prompt = \"You are a financial analyst who specializes in detecting emotional subtext in earnings calls.\"\n",
        "        \n",
        "        # Get the text fields and clean them\n",
        "        instruction = str(example.get('instruction', '')).strip()\n",
        "        input_text = str(example.get('input', '')).strip()\n",
        "        output_text = str(example.get('output', '')).strip()\n",
        "        \n",
        "        # Create the formatted text - DO NOT add <|begin_of_text|> here, tokenizer will add it\n",
        "        formatted = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{instruction}\\n\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n{output_text}<|im_end|>\"\n",
        "        \n",
        "        return formatted\n",
        "    \n",
        "    def train():\n",
        "        # Log configuration\n",
        "        logger.info(f\"Configuration:\")\n",
        "        logger.info(f\"  Precision: {precision}\")\n",
        "        logger.info(f\"  Quantization: {'Yes' if use_quantization else 'No'}\")\n",
        "        logger.info(f\"  Training type: {'Full fine-tune' if full_finetune else 'LoRA'}\")\n",
        "        logger.info(f\"  Batch size: {per_device_train_batch_size}\")\n",
        "        logger.info(f\"  Gradient accumulation: {gradient_accumulation_steps}\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        download_data()\n",
        "        input_file = os.path.join(input_dir, data_name)\n",
        "        \n",
        "        # Load data\n",
        "        logger.info(\"Loading data...\")\n",
        "        examples = load_jsonl_data(input_file)\n",
        "        \n",
        "        # Filter examples with output\n",
        "        valid_examples = [ex for ex in examples if ex.get('output')]\n",
        "        logger.info(f\"Found {len(valid_examples)} valid examples with output\")\n",
        "        \n",
        "        # Take a subset for testing\n",
        "        if len(valid_examples) > 10000:\n",
        "            valid_examples = valid_examples[:10000]\n",
        "            logger.info(f\"Using subset of {len(valid_examples)} examples for testing\")\n",
        "        \n",
        "        # Format examples\n",
        "        logger.info(\"Formatting examples...\")\n",
        "        formatted_texts = []\n",
        "        for i, example in enumerate(valid_examples):\n",
        "            try:\n",
        "                formatted = format_instruction(example)\n",
        "                formatted_texts.append(formatted)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to format example {i}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        logger.info(f\"Successfully formatted {len(formatted_texts)} examples\")\n",
        "        \n",
        "        # Show first example\n",
        "        if formatted_texts:\n",
        "            logger.info(f\"First example (first 300 chars):\\n{formatted_texts[0][:300]}...\")\n",
        "        \n",
        "        # Load tokenizer and model\n",
        "        logger.info(\"Loading tokenizer and model...\")\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "        import torch\n",
        "        \n",
        "        # Get the actual model cache path from environment variable\n",
        "        # Environment variable pattern: MODEL_CACHE_PATH_META_LLAMA_META_LLAMA_3_1_8B_INSTRUCT\n",
        "        env_var_name = f\"MODEL_CACHE_PATH_{model_id_or_path.upper().replace('/', '_').replace('-', '_').replace('.', '_')}\"\n",
        "        model_cache_path = os.getenv(env_var_name) or os.getenv(\"MODEL_CACHE_DIR\")\n",
        "        \n",
        "        logger.info(f\"Checking environment variable: {env_var_name}\")\n",
        "        logger.info(f\"MODEL_CACHE_PATH from env: {model_cache_path}\")\n",
        "        \n",
        "        if model_cache_path:\n",
        "            actual_model_path = model_cache_path\n",
        "            logger.info(f\"Using model cache path from environment variable: {actual_model_path}\")\n",
        "        else:\n",
        "            # Fallback to old behavior\n",
        "            if model_cache_dir != '@auto-populate-modelcache-path':\n",
        "                actual_model_path = os.path.join(model_cache_dir, model_id_or_path) if not model_id_or_path.startswith('/') else model_id_or_path\n",
        "                logger.info(f\"Loading model from cache dir parameter: {actual_model_path}\")\n",
        "            else:\n",
        "                # Download from HuggingFace\n",
        "                actual_model_path = model_id_or_path\n",
        "                logger.info(f\"Downloading model from HuggingFace: {actual_model_path}\")\n",
        "        \n",
        "        # Debug: Check what's available in the parent directories\n",
        "        if actual_model_path.startswith('/gcs/'):\n",
        "            # List GCS mount root\n",
        "            gcs_root = '/gcs'\n",
        "            if os.path.exists(gcs_root):\n",
        "                logger.info(f\"GCS mount root exists: {gcs_root}\")\n",
        "                try:\n",
        "                    buckets = os.listdir(gcs_root)\n",
        "                    logger.info(f\"Available buckets: {buckets}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Cannot list GCS root: {e}\")\n",
        "            else:\n",
        "                logger.error(f\"GCS mount root does not exist: {gcs_root}\")\n",
        "            \n",
        "            # List model cache directory\n",
        "            cache_dir_parts = actual_model_path.split('/')\n",
        "            if len(cache_dir_parts) >= 4:\n",
        "                # /gcs/bucket-name/model-cache\n",
        "                bucket_path = '/'.join(cache_dir_parts[:3])  # /gcs/bucket-name\n",
        "                cache_path = '/'.join(cache_dir_parts[:4])   # /gcs/bucket-name/model-cache\n",
        "                \n",
        "                if os.path.exists(bucket_path):\n",
        "                    logger.info(f\"Bucket path exists: {bucket_path}\")\n",
        "                    try:\n",
        "                        dirs = os.listdir(bucket_path)\n",
        "                        logger.info(f\"Contents of {bucket_path}: {dirs}\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Cannot list {bucket_path}: {e}\")\n",
        "                \n",
        "                if os.path.exists(cache_path):\n",
        "                    logger.info(f\"Cache path exists: {cache_path}\")\n",
        "                    try:\n",
        "                        dirs = os.listdir(cache_path)\n",
        "                        logger.info(f\"Contents of {cache_path}: {dirs}\")\n",
        "                        \n",
        "                        # Check meta-llama subdirectory if it exists\n",
        "                        meta_llama_path = os.path.join(cache_path, 'meta-llama')\n",
        "                        if os.path.exists(meta_llama_path):\n",
        "                            logger.info(f\"meta-llama directory exists: {meta_llama_path}\")\n",
        "                            try:\n",
        "                                meta_llama_contents = os.listdir(meta_llama_path)\n",
        "                                logger.info(f\"Contents of {meta_llama_path}: {meta_llama_contents}\")\n",
        "                            except Exception as e:\n",
        "                                logger.warning(f\"Cannot list {meta_llama_path}: {e}\")\n",
        "                        else:\n",
        "                            logger.warning(f\"meta-llama directory does not exist at: {meta_llama_path}\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Cannot list {cache_path}: {e}\")\n",
        "        \n",
        "        # Validate that model path exists and is accessible\n",
        "        if not os.path.exists(actual_model_path):\n",
        "            error_msg = f\"Model path does not exist: {actual_model_path}\"\n",
        "            if actual_model_path.startswith('/gcs/'):\n",
        "                error_msg += \"\\n  Possible issues:\"\n",
        "                error_msg += \"\\n  1. GCS bucket not mounted (check CSI driver or gcsfuse)\"\n",
        "                error_msg += \"\\n  2. Model not present in the bucket\"\n",
        "                error_msg += \"\\n  3. Incorrect path (check model_cache_dir and model_id_or_path)\"\n",
        "            logger.error(error_msg)\n",
        "            raise FileNotFoundError(error_msg)\n",
        "        \n",
        "        # Check if directory contains model files\n",
        "        model_files = os.listdir(actual_model_path) if os.path.isdir(actual_model_path) else []\n",
        "        logger.info(f\"Found {len(model_files)} files/directories in model path\")\n",
        "        if model_files:\n",
        "            logger.info(f\"Sample files: {model_files[:5]}\")\n",
        "        \n",
        "        # Check for essential model files\n",
        "        essential_files = ['config.json', 'tokenizer_config.json']\n",
        "        missing_files = [f for f in essential_files if not os.path.exists(os.path.join(actual_model_path, f))]\n",
        "        if missing_files:\n",
        "            logger.warning(f\"Missing essential files: {missing_files}\")\n",
        "            logger.warning(\"Model loading may fail or fall back to downloading from HuggingFace\")\n",
        "        \n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            actual_model_path, \n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\"\n",
        "        \n",
        "        # Determine torch dtype based on precision\n",
        "        if precision == \"fp32\":\n",
        "            dtype = torch.float32\n",
        "            logger.info(\"Using FP32 precision\")\n",
        "        elif precision == \"fp16\":\n",
        "            dtype = torch.float16\n",
        "            logger.info(\"Using FP16 precision\")\n",
        "        else:  # bf16\n",
        "            dtype = torch.bfloat16\n",
        "            logger.info(\"Using BF16 precision\")\n",
        "        \n",
        "        # Setup model loading arguments\n",
        "        # NOTE: device_map=\"auto\" causes meta tensor errors with Trainer\n",
        "        # Only use device_map for quantized models (required for quantization)\n",
        "        model_kwargs = {\n",
        "            \"trust_remote_code\": True,\n",
        "            \"torch_dtype\": dtype,\n",
        "        }\n",
        "        \n",
        "        # Add quantization config if requested\n",
        "        # Only nf4 quantization is supported (via bitsandbytes)\n",
        "        if use_quantization and quantization_type == 'nf4':\n",
        "            logger.info(\"Using 4-bit quantization...\")\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=dtype,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "            model_kwargs[\"quantization_config\"] = bnb_config\n",
        "            # device_map is required for quantized models\n",
        "            model_kwargs[\"device_map\"] = \"auto\"\n",
        "            logger.info(\"Using device_map='auto' for quantized model\")\n",
        "        else:\n",
        "            logger.info(\"Loading model without quantization...\")\n",
        "            # For non-quantized models, let Trainer handle device placement\n",
        "            # Using low_cpu_mem_usage to reduce memory during loading\n",
        "            model_kwargs[\"low_cpu_mem_usage\"] = True\n",
        "        \n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            actual_model_path,\n",
        "            **model_kwargs\n",
        "        )\n",
        "        \n",
        "\n",
        "        \n",
        "        # Setup PEFT (only if not doing full fine-tuning)\n",
        "        if not full_finetune:\n",
        "            logger.info(\"Setting up LoRA...\")\n",
        "            from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
        "            \n",
        "            model.config.use_cache = False\n",
        "            \n",
        "            # Prepare for k-bit training if using quantization\n",
        "            if use_quantization and quantization_type == 'nf4':\n",
        "                model = prepare_model_for_kbit_training(model)\n",
        "            else:\n",
        "                # For non-quantized models, enable gradient checkpointing\n",
        "                model.gradient_checkpointing_enable()\n",
        "            \n",
        "            peft_config = LoraConfig(\n",
        "                r=lora_rank,\n",
        "                lora_alpha=lora_alpha,\n",
        "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                lora_dropout=0.05,\n",
        "                bias=\"none\",\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "            )\n",
        "            \n",
        "            model = get_peft_model(model, peft_config)\n",
        "            model.print_trainable_parameters()\n",
        "        else:\n",
        "            logger.info(\"Setting up full fine-tuning...\")\n",
        "            model.config.use_cache = False\n",
        "            model.gradient_checkpointing_enable()\n",
        "            \n",
        "            # Count total parameters\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            logger.info(f\"Total parameters: {total_params:,}\")\n",
        "            logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        \n",
        "        # Tokenize data\n",
        "        logger.info(\"Tokenizing data...\")\n",
        "        from datasets import Dataset\n",
        "        \n",
        "        def tokenize_function(examples):\n",
        "            # Tokenize the texts\n",
        "            model_inputs = tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                padding=False,\n",
        "                return_tensors=None,\n",
        "                add_special_tokens=True,\n",
        "            )\n",
        "            \n",
        "            # For causal language modeling, labels are the same as input_ids\n",
        "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "            return model_inputs\n",
        "        \n",
        "        # Create dataset and tokenize\n",
        "        dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function, \n",
        "            batched=True, \n",
        "            remove_columns=[\"text\"],\n",
        "            desc=\"Tokenizing data\"\n",
        "        )\n",
        "        \n",
        "        # Filter out sequences that are too short (less than 10 tokens)\n",
        "        def filter_short_sequences(example):\n",
        "            return len(example[\"input_ids\"]) >= 10\n",
        "        \n",
        "        tokenized_dataset = tokenized_dataset.filter(filter_short_sequences)\n",
        "        logger.info(f\"Dataset size after filtering short sequences: {len(tokenized_dataset)}\")\n",
        "        \n",
        "        # Check tokenized data\n",
        "        logger.info(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
        "        if len(tokenized_dataset) > 0:\n",
        "            # Check sequence lengths\n",
        "            lengths = [len(example[\"input_ids\"]) for example in tokenized_dataset]\n",
        "            logger.info(f\"Sequence length stats: min={min(lengths)}, max={max(lengths)}, avg={sum(lengths)/len(lengths):.1f}\")\n",
        "            \n",
        "            sample = tokenized_dataset[0]\n",
        "            logger.info(f\"Sample tokenized length: {len(sample['input_ids'])}\")\n",
        "            # Check for duplicate begin tokens\n",
        "            input_ids = sample['input_ids']\n",
        "            if len(input_ids) > 1 and input_ids[0] == input_ids[1] == 128000:\n",
        "                logger.warning(\"Found duplicate <|begin_of_text|> tokens!\")\n",
        "            \n",
        "            # Decode to check format\n",
        "            decoded = tokenizer.decode(input_ids[:100], skip_special_tokens=False)\n",
        "            logger.info(f\"Sample decoded (first 100 tokens): {decoded}\")\n",
        "        \n",
        "        if len(tokenized_dataset) == 0:\n",
        "            logger.error(\"No valid tokenized examples found!\")\n",
        "            return\n",
        "        \n",
        "        # Setup training\n",
        "        logger.info(\"Setting up training...\")\n",
        "        from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "        \n",
        "        # Configure training arguments based on precision\n",
        "        training_kwargs = {\n",
        "            \"output_dir\": output_dir,\n",
        "            \"num_train_epochs\": num_train_epochs,\n",
        "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"logging_steps\": 10,\n",
        "            \"save_steps\": 500,\n",
        "            \"gradient_checkpointing\": True,\n",
        "            \"report_to\": \"none\",\n",
        "        }\n",
        "        \n",
        "        # Set precision-specific arguments\n",
        "        if precision == \"fp32\":\n",
        "            # No special precision flags for FP32\n",
        "            pass\n",
        "        elif precision == \"fp16\":\n",
        "            training_kwargs[\"fp16\"] = True\n",
        "        else:  # bf16\n",
        "            training_kwargs[\"bf16\"] = True\n",
        "        \n",
        "        training_args = TrainingArguments(**training_kwargs)\n",
        "        \n",
        "        # Custom data collator that handles variable length sequences properly\n",
        "        from transformers.data.data_collator import DataCollatorMixin\n",
        "        from dataclasses import dataclass\n",
        "        from typing import Any, Dict, List, Union\n",
        "        import torch\n",
        "        \n",
        "        @dataclass\n",
        "        class DataCollatorForCausalLM(DataCollatorMixin):\n",
        "            \"\"\"\n",
        "            Data collator for causal language modeling that properly handles padding and labels.\n",
        "            \"\"\"\n",
        "            tokenizer: Any\n",
        "            pad_to_multiple_of: int = None\n",
        "            return_tensors: str = \"pt\"\n",
        "            \n",
        "            def torch_call(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "                # Handle the input_ids and labels\n",
        "                batch = {}\n",
        "                \n",
        "                # Get all input_ids and labels\n",
        "                input_ids = [example[\"input_ids\"] for example in examples]\n",
        "                labels = [example[\"labels\"] for example in examples]\n",
        "                \n",
        "                # Pad sequences to the same length\n",
        "                batch[\"input_ids\"] = self._pad_sequences(input_ids, self.tokenizer.pad_token_id)\n",
        "                batch[\"labels\"] = self._pad_sequences(labels, -100)  # -100 is ignored in CrossEntropy loss\n",
        "                \n",
        "                # Create attention mask\n",
        "                batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.tokenizer.pad_token_id).long()\n",
        "                \n",
        "                return batch\n",
        "            \n",
        "            def _pad_sequences(self, sequences: List[List[int]], pad_value: int) -> torch.Tensor:\n",
        "                \"\"\"Pad sequences to the same length.\"\"\"\n",
        "                max_length = max(len(seq) for seq in sequences)\n",
        "                \n",
        "                # Pad to multiple if specified\n",
        "                if self.pad_to_multiple_of is not None:\n",
        "                    max_length = ((max_length + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
        "                \n",
        "                padded_sequences = []\n",
        "                for seq in sequences:\n",
        "                    padded_seq = seq + [pad_value] * (max_length - len(seq))\n",
        "                    padded_sequences.append(padded_seq)\n",
        "                \n",
        "                return torch.tensor(padded_sequences, dtype=torch.long)\n",
        "        \n",
        "        data_collator = DataCollatorForCausalLM(\n",
        "            tokenizer=tokenizer,\n",
        "            pad_to_multiple_of=8,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "        \n",
        "        # Check training setup\n",
        "        train_dataloader = trainer.get_train_dataloader()\n",
        "        logger.info(f\"Training dataloader batches: {len(train_dataloader)}\")\n",
        "        logger.info(f\"Total training steps: {len(train_dataloader) * num_train_epochs}\")\n",
        "        \n",
        "        # Print memory info if using CUDA\n",
        "        if torch.cuda.is_available():\n",
        "            logger.info(\"\\n--- GPU Memory Before Training ---\")\n",
        "            for i in range(torch.cuda.device_count()):\n",
        "                logger.info(f\"GPU {i} - Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
        "                logger.info(f\"GPU {i} - Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
        "        \n",
        "        if len(train_dataloader) == 0:\n",
        "            logger.error(\"No training batches! Check your data and batch size.\")\n",
        "            return\n",
        "        \n",
        "        # Start training\n",
        "        logger.info(\"Starting training...\")\n",
        "        try:\n",
        "            result = trainer.train()\n",
        "            logger.info(\"Training completed successfully!\")\n",
        "            \n",
        "            # Save model\n",
        "            trainer.save_model()\n",
        "            logger.info(f\"Model saved to {output_dir}\")\n",
        "            \n",
        "            # Print final memory usage\n",
        "            if torch.cuda.is_available():\n",
        "                logger.info(\"\\n--- GPU Memory After Training ---\")\n",
        "                for i in range(torch.cuda.device_count()):\n",
        "                    logger.info(f\"GPU {i} - Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
        "                    logger.info(f\"GPU {i} - Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
        "                    logger.info(f\"GPU {i} - Max Allocated: {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")\n",
        "                    logger.info(f\"GPU {i} - Max Reserved: {torch.cuda.max_memory_reserved(i) / 1024**3:.2f} GB\")\n",
        "            \n",
        "            # Print metrics\n",
        "            if result and hasattr(result, 'metrics'):\n",
        "                logger.info(f\"Final metrics: {result.metrics}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "    \n",
        "    train()\n",
        "    return None\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-section",
      "metadata": {},
      "source": [
        "## Testing\n",
        "\n",
        "Uncomment and run the cell below to test the trainOp function locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-trainop",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test trainOp locally (uncomment to run)\n",
        "# trainOp(\n",
        "#     data_name='financial_sentiment_data-20250715.jsonl',\n",
        "#     model_relative_path='model',\n",
        "#     model_name='llama31-financial-sentiment',\n",
        "#     model_dir='/mnt/pretrained-models/',\n",
        "#     num_train_epochs=0.1,  # Use fractional epoch for quick testing\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=2,\n",
        "#     learning_rate=2e-4,\n",
        "#     max_length=512,  # Shorter for testing\n",
        "#     lora_rank=8,  # Smaller for testing\n",
        "#     lora_alpha=16,\n",
        "#     precision='bf16',\n",
        "#     use_quantization=True,\n",
        "#     full_finetune=False,\n",
        "#     quantization_type='nf4'\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}