#!/usr/bin/env python3
"""
Build script for KFP v2.9 pipeline

This script:
1. Reads trainop.py
2. Generates a complete pipeline.py with trainOp embedded
3. Compiles it to YAML

This way, users can edit trainop.py separately, and the build process
handles the embedding automatically.
"""

import os
import sys

def load_trainop_source():
    """Load trainop source code from notebook, extracting only the trainOp function."""
    import json

    current_dir = os.path.dirname(os.path.abspath(__file__))
    notebook_path = os.path.join(current_dir, 'trainop.ipynb')

    # Check if notebook exists, otherwise fall back to trainop.py
    if os.path.exists(notebook_path):
        with open(notebook_path, 'r') as f:
            notebook = json.load(f)

        # Extract only the cells we need: imports and trainop-function
        source_lines = []
        for cell in notebook['cells']:
            cell_id = cell.get('id', '')
            # Only include the imports and trainop-function cells
            if cell_id in ['imports', 'trainop-function']:
                if cell['cell_type'] == 'code':
                    source = cell.get('source', [])
                    if isinstance(source, list):
                        source_lines.extend(source)
                    else:
                        source_lines.append(source)
                    source_lines.append('\n')  # Add newline between cells

        return ''.join(source_lines)
    else:
        # Fall back to trainop.py if it exists
        trainop_path = os.path.join(current_dir, 'trainop.py')
        if not os.path.exists(trainop_path):
            raise FileNotFoundError(
                f"Neither {notebook_path} nor {trainop_path} found. "
                "Please ensure trainop.ipynb exists."
            )

        with open(trainop_path, 'r') as f:
            content = f.read()

        # Filter out any get_ipython() calls that might have been exported
        lines = content.split('\n')
        filtered_lines = [line for line in lines if 'get_ipython()' not in line]
        return '\n'.join(filtered_lines)


def extract_trainop_params(trainop_source):
    """Extract trainOp function parameters using AST parsing."""
    import ast

    tree = ast.parse(trainop_source)

    # Find trainOp function
    trainop_func = None
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef) and node.name == 'trainOp':
            trainop_func = node
            break

    if not trainop_func:
        raise ValueError("Could not find trainOp function")

    params = []
    args = trainop_func.args

    # Get defaults
    defaults = args.defaults
    num_defaults = len(defaults)
    num_args = len(args.args)

    for i, arg in enumerate(args.args):
        param = {'name': arg.arg}

        # Get type annotation
        if arg.annotation:
            param['type'] = ast.unparse(arg.annotation)
        else:
            param['type'] = 'str'  # default type

        # Get default value
        default_idx = i - (num_args - num_defaults)
        if default_idx >= 0:
            param['default'] = ast.unparse(defaults[default_idx])

        params.append(param)

    return params


def generate_pipeline_with_embedded_trainop():
    """Generate pipeline_kfp29.py with trainOp embedded."""

    # Load trainop source
    trainop_source = load_trainop_source()

    # Extract just the trainOp function (from 'def trainOp' onwards)
    lines = trainop_source.split('\n')
    start_idx = None
    for i, line in enumerate(lines):
        if line.startswith('def trainOp('):
            start_idx = i
            break

    if start_idx is None:
        raise ValueError("Could not find trainOp function in trainop.py")

    trainop_function = '\n'.join(lines[start_idx:])

    # Extract parameters from trainOp using AST
    params = extract_trainop_params(trainop_function)

    # Filter out deploy_to_watsonxai and kfp_output_path (internal parameters)
    pipeline_params = [p for p in params if p['name'] not in ['deploy_to_watsonxai', 'kfp_output_path']]

    # Generate pipeline function signature
    pipeline_sig = ',\n    '.join([
        f"{p['name']}: {p['type']}" + (f" = {p['default']}" if 'default' in p else '')
        for p in pipeline_params
    ])

    # Generate component call arguments
    component_args = ',\n        '.join([f"{p['name']}={p['name']}" for p in pipeline_params])

    # Indent the trainOp function to fit inside the component
    indented_trainop = '\n'.join('    ' + line if line.strip() else ''
                                   for line in trainop_function.split('\n'))

    # Generate the complete pipeline code
    pipeline_code = f'''"""
KFP v2.9 Pipeline - Auto-generated with embedded trainOp

DO NOT EDIT THIS FILE DIRECTLY!
Edit trainop.py instead, then run: python build_pipeline.py
"""

from typing import List
import os

from kfp import dsl
from kfp import compiler

try:
    from kfp import kubernetes
except ImportError:
    raise ImportError(
        "kfp-kubernetes is required but not installed. "
        "Install it with: pip install kfp-kubernetes"
    )

# Environment variables
pvcname = os.environ.get('TINTIN_SESSION_TEMPLATE_PVC_NAME')
generated_pipeline_filename = os.environ.get('TINTIN_SESSION_TEMPLATE_GENERATED_PIPELINE_FILENAME', 'pipeline.yaml')
gpu_type_list_text = os.environ.get('TINTIN_SESSION_TEMPLATE_GPU_TYPE_LIST')
default_image = os.environ.get('TINTIN_SESSION_TEMPLATE_DEFAULT_IMAGE', 'asia-east1-docker.pkg.dev/footprintai-prod/kafeido-mlops/jupyter-pytorch-full-kfpv2:nv22.12')
mountPath = os.environ.get('TINTIN_SESSION_TEMPLATE_MOUNT_PATH', '/home/jovyan')
project_id = os.environ.get('TINTIN_SESSION_TEMPLATE_PROJECT_ID')
minio_endpoint = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_ENDPOINT')
minio_access_key = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_ACCESS_KEY')
minio_secret_key = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_SECRET_KEY')
minio_bucket = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_BUCKET')

# Write to current directory (this file will be in outputs/ already)
generated_pipeline_filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), os.path.basename(generated_pipeline_filename))


@dsl.component(
    base_image=default_image,
    packages_to_install=[
        "kfp==2.9.0",
    ]
)
def train_component(
    output_model: dsl.Output[dsl.Model],
    {pipeline_sig}
):
    """Training component with embedded trainOp from trainop.ipynb"""
    from typing import List

    # trainOp function embedded from trainop.ipynb
{indented_trainop}

    # Call trainOp with KFP output path for automatic GCS upload on Vertex AI
    trainOp(
        {component_args},
        deploy_to_watsonxai=False,
        kfp_output_path=output_model.path
    )


@dsl.pipeline(
    name='ResNet CIFAR-10 Training Pipeline',
    description='PyTorch ResNet training on CIFAR-10'
)
def templated_pipeline_func(
    {pipeline_sig}
):
    """KFP v2.9 Pipeline Definition"""

    train_task = train_component(
        {component_args}
    )

    # Use parameters directly for annotations - convert to string
    kubernetes.add_pod_annotation(train_task, annotation_key='tintin.footprint-ai.com/session-model-relative-path', annotation_value=str(model_relative_path))
    kubernetes.add_pod_annotation(train_task, annotation_key='tintin.footprint-ai.com/session-model-name', annotation_value=str(model_name))

    kubernetes.set_image_pull_secrets(train_task, secret_names=['registry-secret'])
    train_task.set_cpu_request('1')
    train_task.set_cpu_limit('1')
    train_task.set_memory_request('4Gi')
    train_task.set_memory_limit('4Gi')


if __name__ == '__main__':
    compiler.Compiler().compile(
        pipeline_func=templated_pipeline_func,
        package_path=generated_pipeline_filename
    )
    print(f"Pipeline compiled successfully to {{generated_pipeline_filename}}")
'''

    return pipeline_code


def main():
    """Main build script."""
    print("Building KFP v2.9 pipeline...")
    print("1. Loading trainop.py...")

    try:
        # Create outputs directory
        base_dir = os.path.dirname(__file__)
        output_dir = os.path.join(base_dir, 'outputs')
        os.makedirs(output_dir, exist_ok=True)

        pipeline_code = generate_pipeline_with_embedded_trainop()

        print("2. Generating pipeline_kfp29_generated.py...")
        output_file = os.path.join(output_dir, 'pipeline_kfp29_generated.py')
        with open(output_file, 'w') as f:
            f.write(pipeline_code)

        print(f"3. Pipeline code written to {output_file}")
        print("4. Compiling pipeline to YAML...")

        # Run the generated pipeline file to compile it
        import subprocess
        result = subprocess.run(
            [sys.executable, output_file],
            capture_output=True,
            text=True
        )

        # Print the compilation output
        if result.stdout:
            print(result.stdout)

        if result.stderr:
            print("STDERR:", result.stderr)

        if result.returncode != 0:
            print(f"✗ Pipeline compilation failed with exit code {result.returncode}")
            sys.exit(1)

        print("✓ Build complete!")
        print(f"✓ Generated file: outputs/pipeline_kfp29_generated.py")
        print(f"✓ Compiled YAML: outputs/{os.environ.get('TINTIN_SESSION_TEMPLATE_GENERATED_PIPELINE_FILENAME', 'pipeline.yaml')}")

    except subprocess.CalledProcessError as e:
        print(f"✗ Build failed: {e}")
        print("STDOUT:", e.stdout)
        print("STDERR:", e.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"✗ Build failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
