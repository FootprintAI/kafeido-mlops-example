#!/usr/bin/env python3
"""
Build script for KFP v1.8.7 pipeline

This script:
1. Reads trainop.ipynb
2. Generates a complete pipeline.py with trainOp embedded
3. Compiles it to Argo Workflow YAML
"""

import os
import sys

def load_trainop_source():
    """Load trainop source code from notebook, extracting only the trainOp function."""
    import json

    current_dir = os.path.dirname(os.path.abspath(__file__))
    notebook_path = os.path.join(current_dir, 'trainop.ipynb')

    # Check if notebook exists
    if os.path.exists(notebook_path):
        with open(notebook_path, 'r') as f:
            notebook = json.load(f)

        # Extract only the cells we need: imports and trainop-function
        source_lines = []
        for cell in notebook['cells']:
            cell_id = cell.get('id', '')
            # Only include the imports and trainop-function cells
            if cell_id in ['imports', 'trainop-function']:
                if cell['cell_type'] == 'code':
                    source = cell.get('source', [])
                    if isinstance(source, list):
                        source_lines.extend(source)
                    else:
                        source_lines.append(source)
                    source_lines.append('\n')  # Add newline between cells

        return ''.join(source_lines)
    else:
        raise FileNotFoundError(
            f"{notebook_path} not found. "
            "Please ensure trainop.ipynb exists."
        )


def extract_trainop_params(trainop_source):
    """Extract trainOp function parameters using AST parsing."""
    import ast

    tree = ast.parse(trainop_source)

    # Find trainOp function
    trainop_func = None
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef) and node.name == 'trainOp':
            trainop_func = node
            break

    if not trainop_func:
        raise ValueError("Could not find trainOp function")

    params = []
    args = trainop_func.args

    # Get defaults
    defaults = args.defaults
    num_defaults = len(defaults)
    num_args = len(args.args)

    for i, arg in enumerate(args.args):
        param = {'name': arg.arg}

        # Get type annotation
        if arg.annotation:
            param['type'] = ast.unparse(arg.annotation)
        else:
            param['type'] = 'str'  # default type

        # Get default value
        default_idx = i - (num_args - num_defaults)
        if default_idx >= 0:
            param['default'] = ast.unparse(defaults[default_idx])

        params.append(param)

    return params


def generate_pipeline_with_embedded_trainop():
    """Generate pipeline_kfp18.py with trainOp embedded."""

    # Load trainop source
    trainop_source = load_trainop_source()

    # Extract just the trainOp function (from 'def trainOp' onwards)
    lines = trainop_source.split('\n')
    start_idx = None
    for i, line in enumerate(lines):
        if line.startswith('def trainOp('):
            start_idx = i
            break

    if start_idx is None:
        raise ValueError("Could not find trainOp function in trainop.ipynb")

    trainop_function = '\n'.join(lines[start_idx:])

    # Extract parameters from trainOp using AST
    params = extract_trainop_params(trainop_function)

    # Filter out deploy_to_watsonxai (internal parameter)
    pipeline_params = [p for p in params if p['name'] not in ['deploy_to_watsonxai']]

    # Generate function signature
    pipeline_sig = ',\n    '.join([
        f"{p['name']}: {p['type']}" + (f" = {p['default']}" if 'default' in p else '')
        for p in pipeline_params
    ])

    # Generate component call arguments
    component_args = ',\n        '.join([f"{p['name']}={p['name']}" for p in pipeline_params])

    # Indent the trainOp function to fit inside the container op
    indented_trainop = '\n'.join('    ' + line if line.strip() else ''
                                   for line in trainop_function.split('\n'))

    # Generate the complete pipeline code for KFP v1.8.7
    pipeline_code = f'''"""
KFP v1.8.7 Pipeline - Auto-generated with embedded trainOp

DO NOT EDIT THIS FILE DIRECTLY!
Edit trainop.ipynb instead, then run: python build_pipeline.py
"""

from typing import List
import os
import kfp
from kfp import dsl
from kfp import compiler

# Environment variables
pvcname = os.environ.get('TINTIN_SESSION_TEMPLATE_PVC_NAME')
gpu_type_list_text = os.environ.get('TINTIN_SESSION_TEMPLATE_GPU_TYPE_LIST')
default_image = os.environ.get('TINTIN_SESSION_TEMPLATE_DEFAULT_IMAGE', 'asia-east1-docker.pkg.dev/footprintai-prod/kafeido-mlops/jupyter-pytorch-full-kfpv2:nv22.12')
mountPath = os.environ.get('TINTIN_SESSION_TEMPLATE_MOUNT_PATH', '/home/jovyan')
project_id = os.environ.get('TINTIN_SESSION_TEMPLATE_PROJECT_ID')
minio_endpoint = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_ENDPOINT')
minio_access_key = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_ACCESS_KEY')
minio_secret_key = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_SECRET_KEY')
minio_bucket = os.environ.get('TINTIN_SESSION_TEMPLATE_MINIO_BUCKET')

# Get the output path from environment variable
output_dir = "outputs"
os.makedirs(output_dir, exist_ok=True)

output_filename = os.getenv("TINTIN_SESSION_TEMPLATE_GENERATED_PIPELINE_FILENAME", "generatedPipelineYaml.yaml")
generated_pipeline_filename = os.path.join(output_dir, output_filename)


def train_op(
    {pipeline_sig}
):
    """Training operation container."""
    from typing import List

    # trainOp function embedded from trainop.ipynb
{indented_trainop}

    # Call trainOp
    trainOp(
        {component_args},
        deploy_to_watsonxai=False
    )


# Create container operation
train_container_op = kfp.components.func_to_container_op(
    train_op,
    base_image=default_image,
    packages_to_install=[
        "kfp==1.8.7",
    ]
)


@dsl.pipeline(
    name='ResNet CIFAR-10 Training Pipeline (KFP v1.8.7)',
    description='PyTorch ResNet training on CIFAR-10'
)
def training_pipeline(
    {pipeline_sig}
):
    """KFP v1.8.7 Pipeline Definition"""

    train_task = train_container_op(
        {component_args}
    )

    # KFP v1.8.7 style configuration - annotations use parameter values directly
    train_task.add_pod_annotation(
        'tintin.footprint-ai.com/session-model-relative-path',
        model_relative_path
    )
    train_task.add_pod_annotation(
        'tintin.footprint-ai.com/session-model-name',
        model_name
    )

    # Set resource limits
    train_task.set_cpu_request('1')
    train_task.set_cpu_limit('1')
    train_task.set_memory_request('4Gi')
    train_task.set_memory_limit('4Gi')


if __name__ == '__main__':
    # KFP v1.8.7 uses Argo workflow compiler
    compiler.Compiler().compile(
        pipeline_func=training_pipeline,
        package_path=generated_pipeline_filename
    )
    print(f"Pipeline compiled successfully to {{generated_pipeline_filename}}")
'''

    return pipeline_code


def main():
    """Main build script."""
    print("Building KFP v1.8.7 pipeline...")
    print("1. Loading trainop.ipynb...")

    try:
        # Create outputs directory
        base_dir = os.path.dirname(__file__)
        output_dir = os.path.join(base_dir, 'outputs')
        os.makedirs(output_dir, exist_ok=True)

        pipeline_code = generate_pipeline_with_embedded_trainop()

        print("2. Generating pipeline_kfp18_generated.py...")
        output_file = os.path.join(output_dir, 'pipeline_kfp18_generated.py')
        with open(output_file, 'w') as f:
            f.write(pipeline_code)

        print(f"3. Pipeline code written to {output_file}")
        print("4. Compiling pipeline to YAML...")

        # Run the generated pipeline file to compile it
        import subprocess
        result = subprocess.run(
            [sys.executable, output_file],
            capture_output=True,
            text=True
        )

        # Print the compilation output
        if result.stdout:
            print(result.stdout)

        if result.stderr:
            print("STDERR:", result.stderr)

        if result.returncode != 0:
            print(f"✗ Pipeline compilation failed with exit code {result.returncode}")
            sys.exit(1)

        print("✓ Build complete!")
        print(f"✓ Generated file: outputs/pipeline_kfp18_generated.py")
        print(f"✓ Compiled YAML: outputs/{os.environ.get('TINTIN_SESSION_TEMPLATE_GENERATED_PIPELINE_FILENAME', 'pipeline.yaml')}")

    except subprocess.CalledProcessError as e:
        print(f"✗ Build failed: {e}")
        print("STDOUT:", e.stdout)
        print("STDERR:", e.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"✗ Build failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
